{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Continuous Normalizing Flow\n",
    "- Neural ODE\n",
    "\n",
    "- Continuous Normalizing Flow"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6a493de071ceb84"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural ODE\n",
    " \n",
    "\n",
    "---\n",
    "\n",
    "# **1. 传统神经网络的计算方式**\n",
    "在普通的神经网络（如 ResNet）中，我们的计算方式是**离散的**：\n",
    "$\\[\n",
    "h_{t+1} = h_t + f(h_t, \\theta)\n",
    "\\]$\n",
    "这里：\n",
    "- $\\( h_t \\)$ 表示**第 \\( t \\) 层的隐藏状态（hidden state）**，即神经网络的输出。\n",
    "- $\\( f(h_t, \\theta) \\)$ 是一个神经网络，表示数据的变化。\n",
    "- 这个公式表示：**每一层的计算都是一个离散跳跃**，从 $\\( h_t \\)$ 跳到 $\\( h_{t+1} \\)$。\n",
    "\n",
    "但这样做的缺点是：\n",
    "- 层数（depth）是固定的，不能动态调整。\n",
    "- 计算是离散的，不能描述连续的变化。\n",
    "\n",
    "---\n",
    "\n",
    "# **2. Neural ODE 的数学表达**\n",
    "Neural ODE 的核心思想是：**把离散的计算过程变成连续的变化**，用**常微分方程（ODE）**描述神经网络的演化过程：\n",
    "$\\[\n",
    "\\frac{dh(t)}{dt} = f(h(t), t, \\theta)\n",
    "\\]$\n",
    "这是一个**微分方程**，表示 $\\( h(t) \\)$ 在连续时间 $\\( t \\)$上的变化规律。\n",
    "\n",
    "## **2.1 这是什么意思？**\n",
    "- **$\\( h(t) \\)$ 是一个连续时间的隐藏状态**，不像传统网络那样按照层的编号 $\\( h_1, h_2, \\dots \\)$ 来索引。\n",
    "- **$\\( f(h, t, \\theta) \\)$ 是一个神经网络**，它告诉我们**隐藏状态如何随时间变化**。\n",
    "- **$\\( \\frac{dh}{dt} \\)$ 代表变化率**，可以理解为 ResNet 的离散差分公式：\n",
    "  $\\[\n",
    "  h_{t+1} - h_t = f(h_t, \\theta)\n",
    "  \\]$\n",
    "  变成连续版本：\n",
    "  $\\[\n",
    "  \\frac{dh}{dt} = f(h, t, \\theta)\n",
    "  \\]$\n",
    "  这表示 $\\( h(t) \\)$ 在时间上的变化不再是固定的离散步长，而是一个**连续的过程**。\n",
    "\n",
    "---\n",
    "\n",
    "## **3. 计算隐藏状态 \\( h(T) \\)**\n",
    "在传统神经网络中，我们通过多个层的计算来得到最终的输出：\n",
    "$\\[\n",
    "h_T = h_0 + \\sum_{t=0}^{T-1} f(h_t, \\theta)\n",
    "\\]$\n",
    "在 Neural ODE 中，我们用微积分的思想，把这个求和变成积分：\n",
    "$\\[\n",
    "h(T) = h(0) + \\int_{0}^{T} f(h(t), t, \\theta) dt\n",
    "\\]$\n",
    "这就是**微分方程的求解**！\n",
    "\n",
    "### **3.1 这是什么意思？**\n",
    "- 我们知道 **$\\( \\frac{dh}{dt} = f(h, t, \\theta) \\)$**，表示 $\\( h(t) \\)$ 的变化速度。\n",
    "- 为了计算 $\\( h(T) \\)$，我们需要在时间区间 $\\( [0, T] \\)$ 内把所有的变化累加起来，这就是积分的作用。\n",
    "- 这意味着 Neural ODE 通过**求解微分方程（ODE Solver）** 来得到最终的隐藏状态 $\\( h(T) \\)$。\n",
    "\n",
    "### **3.2 用 ODE 求解器计算 $\\( h(T) \\)$**\n",
    "由于这个方程没有显式解，我们通常用 **数值方法（Numerical Methods）** 来求解，比如：\n",
    "- **Euler 方法（欧拉法）**：最简单的方式，把时间离散化：\n",
    "  $\\[\n",
    "  h(t + \\Delta t) = h(t) + \\Delta t \\cdot f(h, t, \\theta)\n",
    "  \\]$\n",
    "- **Runge-Kutta 方法（常用的 ODE 求解器）**，更精确。\n",
    "- **Adaptive ODE Solver（自适应 ODE 求解器）**，可以根据误差动态调整步长，提高效率。\n",
    "\n",
    "在代码实现时，我们通常调用 PyTorch 的 `torchdiffeq.odeint` 来求解：\n",
    "```python\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "h_T = odeint(f, h_0, t, theta)  # 计算从 h(0) 到 h(T) 的演化\n",
    "```\n",
    "这里 `odeint` 会自动调用 ODE 求解器，计算 $\\( h(T) \\)$。\n",
    "\n",
    "---\n",
    "\n",
    "# **4. 直观理解 Neural ODE**\n",
    "我们可以把 Neural ODE 形象地理解为**粒子在力场中的运动**：\n",
    "- 传统神经网络是一条**固定的阶梯**，每一层都是一个离散的跳跃。\n",
    "- Neural ODE 是一个**平滑的轨迹**，数据点像粒子一样在**连续的向量场（vector field）**中流动。\n",
    "\n",
    "一个典型的示例是 Normalizing Flows（如 FFJORD），它用 Neural ODE 让数据点在一个流体场中平滑变换，如下图：\n",
    "\n",
    "📈 **数据流动的可视化**\n",
    "（左：普通流模型，右：Neural ODE 生成的数据流）\n",
    "\n",
    "```\n",
    "传统流模型：\n",
    "o-----> o -----> o -----> o\n",
    "\n",
    "Neural ODE：\n",
    "o---o---o---o---o  （连续变化）\n",
    "```\n",
    "\n",
    "这种连续变化可以让模型更自然地学习数据的分布，提高生成能力。\n",
    "\n",
    "---\n",
    "\n",
    "# **5. Neural ODE vs. 传统神经网络**\n",
    "| 特性 | 传统神经网络 | Neural ODE |\n",
    "|------|------------|-------------|\n",
    "| 计算方式 | 通过离散层计算 \\( h_{t+1} = h_t + f(h_t, \\theta) \\) | 通过 ODE 求解器计算 \\( h(T) = h(0) + \\int_{0}^{T} f(h, t, \\theta) dt \\) |\n",
    "| 结构 | 固定层数 | 动态计算，层数不固定 |\n",
    "| 内存占用 | 需要存储所有中间层 | 只存储初始状态，**内存占用低** |\n",
    "| 适用任务 | 传统分类、回归 | **时间序列建模、生成模型** |\n",
    "\n",
    "---\n",
    "\n",
    "# **6. 总结**\n",
    "1. **Neural ODE 通过连续时间微分方程建模神经网络的变化**：\n",
    "   $\\[\n",
    "   \\frac{dh}{dt} = f(h, t, \\theta)\n",
    "   \\]$\n",
    "   **这个公式的作用**：\n",
    "   - 让隐藏状态 $\\( h(t) \\)$ 变成一个连续函数，而不是固定的层。\n",
    "   - 计算 $\\( h(T) \\)$ 时，不是用固定层数的计算，而是求解一个 ODE。\n",
    "  \n",
    "2. **计算方式**：用 ODE 求解器计算隐藏状态的演化：\n",
    "   $\\[\n",
    "   h(T) = h(0) + \\int_{0}^{T} f(h, t, \\theta) dt\n",
    "   \\]$\n",
    "\n",
    "3. **Neural ODE 的优势**：\n",
    "   - **计算更加灵活**，层数可以动态调整，而不是固定的网络结构。\n",
    "   - **内存占用更低**，不需要存储所有中间层。\n",
    "   - **适用于时间序列建模、流体动力学、生成模型（如 FFJORD）**。\n",
    "\n",
    "如果你还不明白，可以想象：\n",
    "- 传统神经网络 = **楼梯**（每一层是一个固定的台阶）。\n",
    "- Neural ODE = **滑梯**（隐藏状态连续变化，没有固定的层数）。\n",
    "\n",
    "这样，Neural ODE 就像一个**平滑的流动过程**，而不是跳跃式的计算！🚀"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69c10fc68a6176de"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c0edc4c420b9314a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b62778ab159557d0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c8afca0511bc0a48"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "328f08df82ef3a60"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
